---
layout: wiki
title: AI / LLM Security Audits
cate1: AI
cate2: 
description: AI / LLM Security Audits
keywords: AI
---

# AI / LLM Security Auditing
It is common to see AI integrations into web applications to generate content, create images or perform other tasks that help simplify processes for end users. These integrations have an LLM backend, which an attacker may be able to interact with to perform malicious actions such as prompt injection, XSS, or other serious consequences. This wiki attempts to detail the many attacks when auditing AI integrations.

## Prompt Injection
Prompt injection is a security vulnerability in AI systems, especially in large language models. It occurs when a malicious actor manipulates the input (prompt) given to the AI to make it perform unintended actions. 

This can involve injecting malicious commands, misleading information, or specially crafted prompts that exploit the model's behavior to gain unauthorized access to data or to make the AI output harmful or unintended results.

